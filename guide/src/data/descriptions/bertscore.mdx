import Typography from "@mui/material/Typography";
import Link from "@mui/material/Link";
import ReactMarkdown from "react-markdown";
import LaTeX from "@site/src/components/LaTeX/LaTeX";

<Typography variant="body2">
  BERTScore uses the pre-trained contextual embeddings from BERT (Bidirectional
  Encoder Representations from Transformers) and similar models to compare a
  candidate text against a ground-truth reference text. This is done by
  computing embeddings for all tokens in both texts and calculating their
  pairwise cosine similarity.
</Typography>

<Typography variant="body2">
  The texts are then compared using greedy matching on the pairwise similarity
  scores, producing recall (based on tokens from the candidate text most similar
  to those from the reference), precision (based on tokens from the reference
  most similar to the candidate text), and an F1 score. This approach allows
  BERTScore to better capture semantic meaning and handle paraphrasing compared
  to n-gram based metrics like BLEU or ROUGE. Importance weighting can also be
  applied, e.g., using inverse document frequency (IDF) scores to give more
  weight to rare words.
</Typography>

<Typography variant="body2" gutterBottom>
  While potentially more powerful than traditional statistical metrics,
  BERTScore still requires a reference text and its performance can depend on
  the specific BERT model used and the layers from which embeddings are
  extracted. It can also only capture semantic overlap between the individual
  tokens and not the overall meaning of the text, which makes it only poorly
  correlated with human judgement when assessing correctness.
</Typography>

<Typography variant="subtitle1" gutterBottom>
  Calculation
</Typography>

<Typography variant="body2">

1. **Token Embeddings**
   - Obtain contextual embeddings for each token in the
     reference text <LaTeX>{"R = (r_1, ..., r_k)"}</LaTeX> and candidate text{" "}
     <LaTeX>{"C = (c_1, ..., c_m)"}</LaTeX> using a pre-trained BERT model.
1. **Cosine Similarity**
   - Compute the cosine similarity between each token in the candidate and
     reference texts,
     <LaTeX>{"\\text{sim}(c_i, r_j) = \\mathbf{r}_i^T \\mathbf{c}_j"}</LaTeX>
     (when using pre-normalised vectors).
1. **BERTScore Recall (<LaTeX>{"R\_{\\text{BERT}}"}</LaTeX>)**
   - Average of the maximum similarity scores for each token in the candidate text,
     optionally weighted by IDF:
     <LaTeX>
       {
         "R_{\\text{BERT}} = \\frac{1}{|R|} \\sum_{r_j \\in R} \\max_{c_i \\in C} \\mathbf{r}_j^T \\mathbf{c}_i"
       }
     </LaTeX>
1. **BERTScore Precision (<LaTeX>{"P\_{\\text{BERT}}"}</LaTeX>)**
   - Average of
     the maximum similarity scores for each token in the reference text, optionally
     weighted by IDF:
     <LaTeX>
       {
         "P_{\\text{BERT}} = \\frac{1}{|C|} \\sum_{c_i \\in C} \\max_{r_j \\in R} \\mathbf{c}_i^T \\mathbf{r}_j"
       }
     </LaTeX>
1. **BERTScore F1 (<LaTeX>{"F1\_{\\text{BERT}}"}</LaTeX>)**
   - The harmonic mean
     of <LaTeX>{"P*{\\text{BERT}}"}</LaTeX> and <LaTeX>{"R*{\\text{BERT}}"}</LaTeX>
     :
     <LaTeX>
       {
         "F1_{\\text{BERT}} = 2 \\frac{P_{\\text{BERT}} \\cdot R_{\\text{BERT}}}{P_{\\text{BERT}} + R_{\\text{BERT}}}"
       }
     </LaTeX>

</Typography>
