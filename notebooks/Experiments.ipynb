{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvalSense Demo Notebook\n",
    "\n",
    "This notebooks illustrates a basic application of EvalSense to the ACI-Bench dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using EvalSense, you may need to configure several environment variables, especially when working with local models. Some commonly used variables are described below:\n",
    "\n",
    "- `EVALSENSE_STORAGE_DIR` — Specifies the directory where EvalSense should store datasets, models, logs ad results. If not set, it defaults to a platform-specific application cache folder.\n",
    "- `CUDA_HOME` — May be required when using vLLM local models with NVIDIA GPUs. If not already defined in your environment, this should point to the root of your CUDA installation (e.g., `/usr/local/cuda-12.4.0`).\n",
    "- `TORCH_CUDA_ARCH_LIST` — Helps suppress CUDA-related warnings when using local models with NVIDIA GPUs. You can determine the appropriate value(s) by running:\n",
    "  ```\n",
    "  nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "  ```\n",
    "- `TOKENIZERS_PARALLELISM` — It is recommended to set this to `true` when using local models. However, if you encounter issues related to parallelism, you may need to set it to `false`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"EVALSENSE_STORAGE_DIR\"] = \"/vol/bitbucket/ad5518/evalsense_cache\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/vol/cuda/12.4.0/\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "As a first step, we need to define a dataset. In this case, we will use the built-in [ACI-Bench dataset](https://www.nature.com/articles/s41597-023-02487-3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Steps\n",
    "\n",
    "The next step is to define how the data should be used to generate model outputs, including the prompts involved. This is achieved using the Solvers abstraction provided by the Inspect AI library. A list of available solvers can be found in the [official documentation](https://inspect.aisi.org.uk/solvers.html#built-in-solvers).\n",
    "\n",
    "Solvers can be composed into a sequence, allowing multiple steps to be chained together, as illustrated below. The full generation steps need to be assigned a unique name — this enables the comparison of results across different experiments when varying prompts or solver configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import generate, prompt_template, system_message\n",
    "\n",
    "from evalsense.generation import GenerationSteps\n",
    "\n",
    "system_prompt_template = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "user_prompt_template = \"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{prompt}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "\n",
    "aci_generation = GenerationSteps(\n",
    "    name=\"Structured\",\n",
    "    steps=[\n",
    "        system_message(system_prompt_template),\n",
    "        prompt_template(user_prompt_template),\n",
    "        generate(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Spec\n",
    "\n",
    "In addition to selecting the dataset to be used, it is also necessary to specify which columns will serve as inputs, targets (i.g., ground-truth references, if available) and sample IDs. Optionally, additional columns can be retained as metadata — for example, if they are useful for the result analysis.\n",
    "\n",
    "Some datasets may support multiple tasks, in which case a preprocessing step may be needed to adapt the data for a specific task. In this case, no task-specific adjustments are needed, so we use the default preprocessor that returns the data unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import FieldSpec\n",
    "\n",
    "from evalsense.tasks import DefaultTaskPreprocessor\n",
    "\n",
    "aci_field_spec = FieldSpec(\n",
    "    input=\"dialogue\",\n",
    "    target=\"note\",\n",
    "    id=\"id\",\n",
    "    metadata=[\n",
    "        \"dataset\",\n",
    "        \"encounter_id\",\n",
    "        \"doctor_name\",\n",
    "        \"patient_gender\",\n",
    "        \"patient_age\",\n",
    "        \"patient_firstname\",\n",
    "        \"patient_familyname\",\n",
    "        \"cc\",\n",
    "        \"2nd_complaints\",\n",
    "    ],\n",
    ")\n",
    "dialogue_task_preprocessor = DefaultTaskPreprocessor(name=\"Dialogue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config\n",
    "\n",
    "Next, we specify the model(s) to be used in our experiment. We can use any model and model provider supported by Inspect AI, configurable through model arguments and generation configuration. For an overview of supported models and the arguments, please refer to the related [Inspect AI documentation](https://inspect.aisi.org.uk/models.html). In this demo, we use two local vLLM models — [Llama 3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B) and [Phi 4 Mini Instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import GenerateConfigArgs\n",
    "\n",
    "from evalsense.constants import MODELS_PATH\n",
    "from evalsense.generation import ModelConfig\n",
    "\n",
    "gemma_config = ModelConfig(\n",
    "    \"vllm/google/gemma-3-27b-it\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "qwen_config = ModelConfig(\n",
    "    \"vllm/Qwen/Qwen2.5-14B-Instruct-1M\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "llama_config = ModelConfig(\n",
    "    \"vllm/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "phi_config = ModelConfig(\n",
    "    \"vllm/microsoft/Phi-4-mini-instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this step, we define the evaluation metrics. Here, we simply use the BLUE, ROUGE and BERTScore metrics as implemented in EvalSense. Custom metrics are also supported, provided that they are implemented as Inspect AI-compatible [scorers](https://inspect.aisi.org.uk/scorers.html#custom-scorers) and [metrics](https://inspect.aisi.org.uk/reference/inspect_ai.scorer.html#metric). For reference and inspiration, please check the [EvalSense BLUE implementation](https://github.com/nhsengland/evalsense/blob/main/evalsense/evaluation/evaluators/bleu.py) and the [relevant Inspect AI documentation](https://inspect.aisi.org.uk/scorers.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import (\n",
    "    get_bertscore_evaluator,\n",
    "    get_bleu_evaluator,\n",
    "    get_rouge_evaluator,\n",
    ")\n",
    "\n",
    "bleu_evaluator = get_bleu_evaluator()\n",
    "rouge_evaluator = get_rouge_evaluator()\n",
    "bertscore_evaluator = get_bertscore_evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the use of more advanced metrics, we also include [G-Eval](https://arxiv.org/abs/2303.16634), a LLM-as-a-Judge approach that uses structured propts and weights judge scores based on token output proabilities, resulting in more stable evaluation scores. Unlike simpler metrics, G-Eval requires additional configuration, such as a function for constructing the prompts as well as additional hyperparameters.\n",
    "\n",
    "Since G-Eval is a model-based metric, we also need to supply a model configuration so that EvalSense can instantiate the appropriate model during evaluation. Internally, EvalSense handles this evaluator differently from simpler metrics by using a `ScorerFactory` — an abstraction that creates a scorer when passed an Inspect AI model. This design allows LLMScore to optimise resource usage by loading the model only when needed and releasing the associated resources afterwards.\n",
    "\n",
    "While you don't need to worry about these internal details if you are only using default EvalSense evaluators, understanding them can be useful if you want to implement your own model-based evaluation methods. For reference, you can check the [EvalSense implementation of G-Eval](https://github.com/nhsengland/evalsense/blob/main/evalsense/evaluation/evaluators/g_eval.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import get_g_eval_evaluator\n",
    "\n",
    "\n",
    "detailed_prompt = \"\"\"You are a medical expert tasked with evaluating the faithfulness of a clinical note generated by a model from doctor-patient dialogue. You will be given:\n",
    "\n",
    "* The reference note produced by a human expert\n",
    "* The candidate note produced by the model\n",
    "\n",
    "Your goal is to determine whether the candidate note is faithful to the reference note, using the following evaluation criteria:\n",
    "\n",
    "1. **Accuracy of Medical Facts**: All medical conditions, diagnoses, treatments, and test results must be correctly stated and consistent with the source text.\n",
    "2. **Completeness of Critical Information**: The note should include all vital information necessary for follow-up care (e.g., key symptoms, diagnoses, procedures, outcomes).\n",
    "3. **Absence of Hallucinations**: The note should not introduce any information that is not present in the original discharge note.\n",
    "4. **Clarity and Non-Misleading Content**: The note should be clear, free of ambiguity, and should not distort or misrepresent any facts.\n",
    "\n",
    "Instructions:\n",
    "* Compare the candidate note to the reference note.\n",
    "* Provide a numerical rating of the candidate note's faithfulness on a scale from 1 (completely unfaithful) to 10 (fully faithful).\n",
    "* Respond only with the numerical rating without any explanation or context.\n",
    "\n",
    "Reference Note:\n",
    "{reference}\n",
    "\n",
    "Candidate Note:\n",
    "{prediction}\n",
    "\n",
    "Output Format:\n",
    "[Numerical faithfulness rating only, from 1 to 10]\"\"\"\n",
    "\n",
    "\n",
    "brief_prompt = \"\"\"Assess whether the candidate note faithfully and accurately reflects the content of the reference note.\n",
    "\n",
    "Provide a numerical rating of the candidate note's faithfulness on a scale from 1 (completely unfaithful) to 10 (fully faithful). Respond only with the numerical rating without any explanation or context.\n",
    "\n",
    "Reference Note:\n",
    "{reference}\n",
    "\n",
    "Candidate Note:\n",
    "{prediction}\n",
    "\n",
    "Output Format:\n",
    "[Numerical faithfulness rating only, from 1 to 10]\"\"\"\n",
    "\n",
    "\n",
    "llama_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=llama_config,\n",
    ")\n",
    "llama_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=llama_config,\n",
    ")\n",
    "qwen_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Qwen 2.5\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=qwen_config,\n",
    ")\n",
    "qwen_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Qwen 2.5\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=qwen_config,\n",
    ")\n",
    "gemma_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Gemma 3\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=gemma_config,\n",
    ")\n",
    "gemma_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Gemma 3\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=gemma_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as an example of even more sophisticated evaluation approaches, we define the QAGS scorer. This scorer evaluates agreement between the factual content in the reference text and the target text, and requires configuration for several different prompts that are used in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Literal, override\n",
    "\n",
    "from evalsense.evaluation.evaluators.qags import QagsConfig, get_qags_evaluator\n",
    "\n",
    "\n",
    "class TernaryReferenceBasedQagsConfig(QagsConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            answer_comparison_mode=\"ternary\",\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def get_question_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        prediction: str,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"You are an expert medical assistant specialised in processing clinical notes. Your task is to formulate a set of close-ended questions (with yes/no answers) that thoroughly cover the information in the provided clinical note. The questions should be clear, self-contained, unambiguous and directly referring to the key points in the note. You should respond with each question on a new line, without any additional comments or explanations (in particular, you should not provide any answers to the questions).\\n\\n\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += prediction\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Each close-ended question on a separate line, without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        question: str,\n",
    "        prediction: str | None = None,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in answering close-ended questions about clinical notes. Your task is to provide an answer to the below question based on the provided clinical note. The answer should be a single word out of the following options:\n",
    "* Yes\n",
    "* No\n",
    "* Unknown\n",
    "\n",
    "You should reply with unknown if the answer cannot reasonably be determined from the note. You should not provide any additional comments or explanations.\\n\\n\"\"\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += self.enforce_not_none(\"prediction\", prediction)\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\\n\\nQuestion:\\n\" + question\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Single word answer: Yes, No, or Unknown]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class JudgeReferenceBasedQagsConfig(QagsConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            answer_comparison_mode=\"judge\",\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def get_question_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        prediction: str,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"You are an expert medical assistant specialised in processing clinical notes. Your task is to formulate a set of questions that thoroughly cover the information in the provided clinical note. The questions should be clear, self-contained, unambiguous and directly referring to the key points in the note. All questions you devise should only require brief answers (typically a single word or a short phrase). You should respond with each question on a new line, without any additional comments or explanations (in particular, you should not provide any answers to the questions).\\n\\n\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += prediction\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Each question on a separate line, without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        question: str,\n",
    "        prediction: str | None = None,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in answering questions about clinical notes. Your task is to provide an answer to the below question based on the provided clinical note.\n",
    "\n",
    "You should reply with unknown if the answer cannot reasonably be determined from the note. You should not provide any additional comments or explanations — just a direct, succinct answer to the question.\\n\\n\"\"\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += self.enforce_not_none(\"prediction\", prediction)\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\\n\\nQuestion:\\n\" + question\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Succinct answer without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_comparison_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        question: str,\n",
    "        prediction_answer: str,\n",
    "        reference_answer: str,\n",
    "        input: str | None = None,\n",
    "        prediction: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in evaluating answers to questions about clinical notes. Your task is to compare the two answers to the below question and determine whether they convey the same meaning. While the answers may be phrased differently, they should be semantically equivalent. You should respond with a single word answer out of the following options:\n",
    "* Yes\n",
    "* No\n",
    "\n",
    "You should not provide any additional comments or explanations.\\n\\n\"\"\"\n",
    "        prompt += \"Question:\\n\" + question\n",
    "        prompt += \"\\n\\nPrediction Answer:\\n\" + prediction_answer\n",
    "        prompt += \"\\n\\nReference Answer:\\n\" + reference_answer\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Single word answer: Yes or No]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "ternary_reference_based_qags_config = TernaryReferenceBasedQagsConfig()\n",
    "llama_ternary_reference_based_qags_evaluator = get_qags_evaluator(\n",
    "    name=\"Ternary Ref. QAGS\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    config=ternary_reference_based_qags_config,\n",
    "    model_config=llama_config,\n",
    ")\n",
    "judge_reference_based_qags_config = JudgeReferenceBasedQagsConfig()\n",
    "llama_judge_reference_based_qags_evaluator = get_qags_evaluator(\n",
    "    name=\"Judge Ref. QAGS\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    config=judge_reference_based_qags_config,\n",
    "    model_config=llama_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "To bring everything together, we configure the overall task and experiment pipeline. For more extensive and complex evaluations, multiple experiment batch configurations can be specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "\n",
    "aci_task_config = TaskConfig(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    generation_steps=aci_generation,\n",
    "    field_spec=aci_field_spec,\n",
    "    task_preprocessor=dialogue_task_preprocessor,\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=[aci_task_config],\n",
    "    model_configs=[gemma_config, qwen_config, llama_config, phi_config],\n",
    "    evaluators=[\n",
    "        bleu_evaluator,\n",
    "        rouge_evaluator,\n",
    "        bertscore_evaluator,\n",
    "        llama_detailed_g_eval_evaluator,\n",
    "        llama_brief_g_eval_evaluator,\n",
    "        qwen_detailed_g_eval_evaluator,\n",
    "        qwen_brief_g_eval_evaluator,\n",
    "        gemma_detailed_g_eval_evaluator,\n",
    "        gemma_brief_g_eval_evaluator,\n",
    "        llama_ternary_reference_based_qags_evaluator,\n",
    "        llama_judge_reference_based_qags_evaluator,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all the logs, outputs and results associated with our experiments, we also create a new `Project` object, which is passed to the evaluation pipeline. When initialised, the project automatically loads any previously stored outputs and evaluation results assciated with the given project name. This ensures that any completed tasks are not unnecessarily rerun unless explicitly required by setting `force_rerun=True` when calling the pipeline's `run` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Evaluation\")\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "The results associated with the project can be analysed using dedicated result analysers. In this example, we use a simple tabular result analyser, which summarises all the evaluation metrics in a structured table format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from evalsense.workflow.analysers import TabularResultAnalyser\n",
    "\n",
    "analyser = TabularResultAnalyser[pl.DataFrame](output_format=\"polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = analyser(aci_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.workflow.analysers import CorrelationResults, MetricCorrelationAnalyser\n",
    "\n",
    "analyser = MetricCorrelationAnalyser[CorrelationResults[pl.DataFrame]](\n",
    "    output_format=\"polars\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results = analyser(\n",
    "    aci_project,\n",
    "    return_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
