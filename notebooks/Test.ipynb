{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLMSCOPE_CACHE_DIR\"] = \"/vol/bitbucket/ad5518/llmscope_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def aci_task_preprocessor(dataset: Dataset) -> Dataset:\n",
    "    return dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aci_prompt_formatter(dialogue, **kwargs):\n",
    "    system_prompt = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "    user_prompt = f\"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{dialogue}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.llms.managers import VLlmManager\n",
    "\n",
    "llama_llm_manager = VLlmManager(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.pipelines import SimplePipeline\n",
    "\n",
    "aci_pipeline = SimplePipeline(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    task_preprocessor=aci_task_preprocessor,\n",
    "    prompt_formatter=aci_prompt_formatter,\n",
    "    llm_manager=llama_llm_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"dialogue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"note\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.constants import OUTPUT_COLUMN\n",
    "\n",
    "print(outputs[0][OUTPUT_COLUMN])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
