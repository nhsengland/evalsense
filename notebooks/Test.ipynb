{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLMSCOPE_CACHE_DIR\"] = \"/vol/bitbucket/ad5518/llmscope_cache\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/vol/cuda/12.4.0/\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import generate, prompt_template, system_message\n",
    "\n",
    "from llmscope.generation import GenerationSteps\n",
    "\n",
    "system_prompt_template = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "user_prompt_template = \"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{prompt}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "\n",
    "aci_generation = GenerationSteps(\n",
    "    name=\"Structured\",\n",
    "    solver=[\n",
    "        system_message(system_prompt_template),\n",
    "        prompt_template(user_prompt_template),\n",
    "        generate(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import FieldSpec\n",
    "\n",
    "from llmscope.tasks import DefaultTaskPreprocessor\n",
    "\n",
    "aci_field_spec = FieldSpec(\n",
    "    input=\"dialogue\",\n",
    "    target=\"note\",\n",
    "    id=\"id\",\n",
    "    metadata=[\n",
    "        \"dataset\",\n",
    "        \"encounter_id\",\n",
    "        \"doctor_name\",\n",
    "        \"patient_gender\",\n",
    "        \"patient_age\",\n",
    "        \"patient_firstname\",\n",
    "        \"patient_familyname\",\n",
    "        \"cc\",\n",
    "        \"2nd_complaints\",\n",
    "    ],\n",
    ")\n",
    "dialogue_task_preprocessor = DefaultTaskPreprocessor(name=\"Dialogue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import GenerateConfigArgs\n",
    "\n",
    "from llmscope.constants import MODELS_PATH\n",
    "from llmscope.generation import ModelConfig\n",
    "\n",
    "llama_config = ModelConfig(\n",
    "    \"vllm/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"1\",\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation.evaluators import bleu_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "\n",
    "aci_task_config = TaskConfig(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    generation_steps=aci_generation,\n",
    "    field_spec=aci_field_spec,\n",
    "    task_preprocessor=dialogue_task_preprocessor,\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=[aci_task_config],\n",
    "    model_configs=[llama_config],\n",
    "    evaluators=[bleu_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Evaluation\")\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
