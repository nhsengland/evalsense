{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLMSCOPE_CACHE_DIR\"] = \"/vol/bitbucket/ad5518/llmscope_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from llmscope.columns import ColumnConfig\n",
    "from llmscope.tasks import TaskPreprocessor\n",
    "\n",
    "column_config = ColumnConfig()\n",
    "\n",
    "\n",
    "def preprocessing_function(dataset: Dataset) -> Dataset:\n",
    "    dataset = dataset.rename_column(\"note\", column_config.references.column_name)\n",
    "    return dataset.select(range(10))\n",
    "\n",
    "\n",
    "aci_task_preprocessor = TaskPreprocessor(\n",
    "    name=\"Dialogue Summarization\",\n",
    "    preprocessing_function=preprocessing_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Formatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.prompts import PromptFormatter\n",
    "\n",
    "\n",
    "def formatting_function(dialogue, **kwargs):\n",
    "    system_prompt = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "    user_prompt = f\"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{dialogue}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "aci_prompt_formatter = PromptFormatter(\n",
    "    name=\"Generate Note\", formatting_function=formatting_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.llms.managers import VLlmManager\n",
    "\n",
    "llama_llm_manager = VLlmManager(name=\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation.evaluators import BleuEvaluator\n",
    "\n",
    "bleu_evaluator = BleuEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "\n",
    "task_config = TaskConfig(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    task_preprocessor=aci_task_preprocessor,\n",
    "    prompt_formatter=aci_prompt_formatter,\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=[task_config],\n",
    "    llm_managers=[llama_llm_manager],\n",
    "    evaluators=[bleu_evaluator],\n",
    "    column_config=column_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Evaluation\", reset_project=True)\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
