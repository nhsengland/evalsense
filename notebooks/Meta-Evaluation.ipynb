{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Meta-Evaluation Notebook\n",
    "\n",
    "This notebook largely mirrors the `Experiments.ipynb` notebook in terms of the used evaluation methods and general setup, but focuses on meta-evaluation on synthetically perturbed data. That is, it aims to establish which of the available evaluation methods are the most suitable for assessing the performance of LLMs on the given task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Before using EvalSense, you may need to configure several environment variables, especially when working with local models. Some commonly used variables are described below:\n",
    "\n",
    "- `EVALSENSE_STORAGE_DIR` — Specifies the directory where EvalSense should store datasets, logs and results. If not set, it defaults to a platform-specific application cache folder.\n",
    "- `HF_HOME` — Specifies the directory used for storing local Hugging Face models. Depending on your system and preferences, you may wish to change this setting instead of using the default cache folder.\n",
    "- `CUDA_HOME` — May be required when using vLLM local models with NVIDIA GPUs. If not already defined in your environment, this should point to the root of your CUDA installation (e.g., `/usr/local/cuda-12.4.0`).\n",
    "- `TORCH_CUDA_ARCH_LIST` — Helps suppress CUDA-related warnings when using local models with NVIDIA GPUs. You can determine the appropriate value(s) by running:\n",
    "  ```\n",
    "  nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "  ```\n",
    "- `TOKENIZERS_PARALLELISM` — It is recommended to set this to `true` when using local models. However, if you encounter issues related to parallelism, you may need to set it to `false`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/vol/bitbucket/ad5518/hf_home\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/vol/cuda/12.4.0/\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "As a first step, we need to define a dataset. In this case, we will use the built-in [ACI-Bench dataset](https://www.nature.com/articles/s41597-023-02487-3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.datasets import DatasetManager\n",
    "\n",
    "aci_dataset_manager = DatasetManager.create(\n",
    "    \"aci-bench\", splits=[\"test1\", \"test2\", \"test3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Perturbation Prompts\n",
    "\n",
    "In the next step, we specify different \"tiers\" of prompts applying progressively more aggressive perturbations to the outputs. This establishes a ground-truth ranking that we can compare with the scores from different evaluation methods to determine their suitability for assessing a specific criterion on the considered task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiered_perturbation_types = [\n",
    "    [\n",
    "        \"- Rephrase sentences while preserving the exact medical meaning. You may use synonyms, vary sentence structure, or change sentence length, but all clinical facts and measurements must remain unchanged.\",\n",
    "        \"- Slightly alter the writing style, such as using different terminology or presenting findings differently, while ensuring the factual content remains identical.\",\n",
    "    ],\n",
    "    [\n",
    "        \"- Make small changes to test results and quantitative measurements, ensuring they remain clinically plausible and consistent with the original context.\",\n",
    "        \"- Introduce minor modifications to the patient's reported symptoms, making sure they are still consistent with the assessment, diagnosis, and treatment plan (e.g., adding or substituting symptoms that commonly co-occur).\",\n",
    "        \"- Slightly adjust the patient's clinical history, ensuring consistency with the assessment, diagnosis, and treatment plan.\",\n",
    "        \"- Make minor modifications to the treatment plan, but ensure it remains appropriate for the assessment and diagnosis.\",\n",
    "    ],\n",
    "    [\n",
    "        \"- Significantly alter test results and quantitative measurements, in a way that may change the clinical interpretation or implications of the note.\",\n",
    "        \"- Make substantial changes to the patient's reported symptoms, potentially affecting the clinical interpretation of the note.\",\n",
    "        \"- Make substantial changes to the patient's clinical history, potentially affecting the clinical interpretation.\",\n",
    "        \"- Significantly modify the treatment plan, such that it may lead to a different clinical outcome than the original plan.\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "system_prompt_template = \"You are a medical AI assistant. Your role is to generate plausible variations of clinical notes by applying controlled content perturbations.\"\n",
    "\n",
    "user_prompt_template = \"\"\"Your task is to generate a clinically plausible variation of the provided clinical note. \n",
    "\n",
    "You should maintain the original note's structure and formatting, but modify its content according to the specified types of perturbation below. Try to maintain internal consistency and general medical plausibility when applying any changes.\n",
    "\n",
    "**Perturbation Instructions**  \n",
    "Apply the following types of perturbations:\n",
    "{perturbation_types}\n",
    "\n",
    "Respond only with the perturbed clinical note, do not include any commentary, reasoning or explanation.\n",
    "\n",
    "**Original Clinical Note**\n",
    "{prompt}\n",
    "\n",
    "**Perturbed Clinical Note**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Model Config\n",
    "\n",
    "Next, we specify the model(s) to be used in our experiment. We can use any model and model provider supported by Inspect AI, configurable through model arguments and generation configuration. For an overview of supported models and the arguments, please refer to the related [Inspect AI documentation](https://inspect.aisi.org.uk/models.html). In this demo, we use several local vLLM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import GenerateConfigArgs\n",
    "\n",
    "from evalsense.generation import ModelConfig\n",
    "\n",
    "gemma_27_config = ModelConfig(\n",
    "    \"vllm/google/gemma-3-27b-it\",\n",
    "    model_args={\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "qwen_14_config = ModelConfig(\n",
    "    \"vllm/Qwen/Qwen3-14B\",\n",
    "    model_args={\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "llama_config = ModelConfig(\n",
    "    \"vllm/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_args={\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "phi_config = ModelConfig(\n",
    "    \"vllm/microsoft/phi-4\",\n",
    "    model_args={\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this step, we define the evaluation metrics. We use the same evaluation metrics as in the `Experiments.ipynb` notebook. Please refer to this notebook for a more detailed description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import (\n",
    "    get_bertscore_evaluator,\n",
    "    get_bleu_evaluator,\n",
    "    get_rouge_evaluator,\n",
    ")\n",
    "\n",
    "bleu_evaluator = get_bleu_evaluator()\n",
    "rouge_evaluator = get_rouge_evaluator()\n",
    "bertscore_evaluator = get_bertscore_evaluator(device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import get_g_eval_evaluator\n",
    "\n",
    "\n",
    "detailed_prompt = \"\"\"You are a medical expert tasked with evaluating the faithfulness of a clinical note generated by a model from doctor-patient dialogue. You will be given:\n",
    "\n",
    "* The reference note produced by a human expert\n",
    "* The candidate note produced by the model\n",
    "\n",
    "Your goal is to determine whether the candidate note is faithful to the reference note, using the following evaluation criteria:\n",
    "\n",
    "1. **Accuracy of Medical Facts**: All medical conditions, diagnoses, treatments, and test results must be correctly stated and consistent with the source text.\n",
    "2. **Completeness of Critical Information**: The note should include all vital information necessary for follow-up care (e.g., key symptoms, diagnoses, procedures, outcomes).\n",
    "3. **Absence of Hallucinations**: The note should not introduce any information that is not present in the original discharge note.\n",
    "4. **Clarity and Non-Misleading Content**: The note should be clear, free of ambiguity, and should not distort or misrepresent any facts.\n",
    "\n",
    "Instructions:\n",
    "* Compare the candidate note to the reference note.\n",
    "* Provide a numerical rating of the candidate note's faithfulness on a scale from 1 (completely unfaithful) to 5 (fully faithful).\n",
    "* Respond only with the numerical rating without any explanation or context.\n",
    "\n",
    "Reference Note:\n",
    "{reference}\n",
    "\n",
    "Candidate Note:\n",
    "{prediction}\n",
    "\n",
    "Output Format:\n",
    "[Numerical faithfulness rating only, from 1 to 5]\"\"\"\n",
    "\n",
    "\n",
    "brief_prompt = \"\"\"Assess whether the candidate note faithfully and accurately reflects the content of the reference note.\n",
    "\n",
    "Provide a numerical rating of the candidate note's faithfulness on a scale from 1 (completely unfaithful) to 5 (fully faithful). Respond only with the numerical rating without any explanation or context.\n",
    "\n",
    "Reference Note:\n",
    "{reference}\n",
    "\n",
    "Candidate Note:\n",
    "{prediction}\n",
    "\n",
    "Output Format:\n",
    "[Numerical faithfulness rating only, from 1 to 5]\"\"\"\n",
    "\n",
    "\n",
    "llama_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Llama 3.1 8B\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=llama_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")\n",
    "llama_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Llama 3.1 8B\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=llama_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")\n",
    "qwen_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Qwen 3 14B\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=qwen_14_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")\n",
    "qwen_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Qwen 3 14B\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=qwen_14_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")\n",
    "gemma_detailed_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Detail\",\n",
    "    model_name=\"Gemma 3 27B\",\n",
    "    prompt_template=detailed_prompt,\n",
    "    model_config=gemma_27_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")\n",
    "gemma_brief_g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"F. Brief\",\n",
    "    model_name=\"Gemma 3 27B\",\n",
    "    prompt_template=brief_prompt,\n",
    "    model_config=gemma_27_config,\n",
    "    min_score=1,\n",
    "    max_score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Literal, override\n",
    "\n",
    "from evalsense.evaluation.evaluators.qags import QagsConfig, get_qags_evaluator\n",
    "\n",
    "\n",
    "class TernaryReferenceBasedQagsConfig(QagsConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            answer_comparison_mode=\"ternary\",\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def get_question_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        prediction: str,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"You are an expert medical assistant specialised in processing clinical notes. Your task is to formulate a set of close-ended questions (with yes/no answers) that thoroughly cover the information in the provided clinical note. The questions should be clear, self-contained, unambiguous and directly referring to the key points in the note. You should respond with each question on a new line, without any additional comments or explanations (in particular, you should not provide any answers to the questions).\\n\\n\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += prediction\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Each close-ended question on a separate line, without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        question: str,\n",
    "        prediction: str | None = None,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in answering close-ended questions about clinical notes. Your task is to provide an answer to the below question based on the provided clinical note. The answer should be a single word out of the following options:\n",
    "* Yes\n",
    "* No\n",
    "* Unknown\n",
    "\n",
    "You should reply with unknown if the answer cannot reasonably be determined from the note. You should not provide any additional comments or explanations.\\n\\n\"\"\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += self.enforce_not_none(\"prediction\", prediction)\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\\n\\nQuestion:\\n\" + question\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Single word answer: Yes, No, or Unknown]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class JudgeReferenceBasedQagsConfig(QagsConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            answer_comparison_mode=\"judge\",\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def get_question_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        prediction: str,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"You are an expert medical assistant specialised in processing clinical notes. Your task is to formulate a set of questions that thoroughly cover the information in the provided clinical note. The questions should be clear, self-contained, unambiguous and directly referring to the key points in the note. All questions you devise should only require brief answers (typically a single word or a short phrase). You should respond with each question on a new line, without any additional comments or explanations (in particular, you should not provide any answers to the questions).\\n\\n\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += prediction\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Each question on a separate line, without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_generation_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        source: Literal[\"prediction\"] | Literal[\"reference\"],\n",
    "        question: str,\n",
    "        prediction: str | None = None,\n",
    "        input: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in answering questions about clinical notes. Your task is to provide an answer to the below question based on the provided clinical note.\n",
    "\n",
    "You should reply with unknown if the answer cannot reasonably be determined from the note. You should not provide any additional comments or explanations — just a direct, succinct answer to the question.\\n\\n\"\"\"\n",
    "        prompt += \"Provided Note:\\n\"\n",
    "        if source == \"prediction\":\n",
    "            prompt += self.enforce_not_none(\"prediction\", prediction)\n",
    "        elif source == \"reference\":\n",
    "            prompt += self.enforce_not_none(\"reference\", reference)\n",
    "        else:\n",
    "            raise ValueError(\"source must be either 'prediction' or 'reference'\")\n",
    "        prompt += \"\\n\\nQuestion:\\n\" + question\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Succinct answer without any additional comments or explanations]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    @override\n",
    "    def get_answer_comparison_prompt(\n",
    "        self,\n",
    "        *,\n",
    "        question: str,\n",
    "        prediction_answer: str,\n",
    "        reference_answer: str,\n",
    "        input: str | None = None,\n",
    "        prediction: str | None = None,\n",
    "        reference: str | None = None,\n",
    "        metadata: dict[str, Any] | None = None,\n",
    "    ) -> str:\n",
    "        prompt = \"\"\"You are an expert medical assistant specialised in evaluating answers to questions about clinical notes. Your task is to compare the two answers to the below question and determine whether they convey the same meaning. While the answers may be phrased differently, they should be semantically equivalent. You should respond with a single word answer out of the following options:\n",
    "* Yes\n",
    "* No\n",
    "\n",
    "You should not provide any additional comments or explanations.\\n\\n\"\"\"\n",
    "        prompt += \"Question:\\n\" + question\n",
    "        prompt += \"\\n\\nPrediction Answer:\\n\" + prediction_answer\n",
    "        prompt += \"\\n\\nReference Answer:\\n\" + reference_answer\n",
    "        prompt += \"\"\"\\n\\nOutput Format:\n",
    "[Single word answer: Yes or No]\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "ternary_reference_based_qags_config = TernaryReferenceBasedQagsConfig()\n",
    "llama_ternary_reference_based_qags_evaluator = get_qags_evaluator(\n",
    "    name=\"Ternary Ref. QAGS\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    config=ternary_reference_based_qags_config,\n",
    "    model_config=llama_config,\n",
    ")\n",
    "judge_reference_based_qags_config = JudgeReferenceBasedQagsConfig()\n",
    "llama_judge_reference_based_qags_evaluator = get_qags_evaluator(\n",
    "    name=\"Judge Ref. QAGS\",\n",
    "    model_name=\"Llama 3.1\",\n",
    "    config=judge_reference_based_qags_config,\n",
    "    model_config=llama_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Here, we specify the experiment configurations for performing the meta-evaluation. We need to define several components for each perturbation tier we specified earlier:\n",
    "\n",
    "- **The `perturbation_record_to_sample`** function acts as a drop-in replacement for the simpler `FieldSpec` that we have been using in some of the other EvalSense notebooks. Instead of only specifying the mapping between the dataset fields and sample fields, using a dedicated function allows us to store the used perturbation tier in each sample's metadata (`\"perturbation_type_tier\": type_tier`). This allows distinguishing between different levels of perturbations in the subsequent analysis.\n",
    "- **The `GenerationSteps`** specify the steps used during the generation to perturb the samples, and use the perturbation prompt templates defined above.\n",
    "- **The `TaskPreprocessor`.** Since we don't need to perform any additonal preprocessing, we simply use the default task preprocessor, customizing its name to clarify that we are perturbing the samples.\n",
    "- **The `TaskConfig`** specifies the generation task to be performed for each tier of perturbations, using the other components described above.\n",
    "\n",
    "Finally, all the tasks, model configurations and evaluators are incorporated in an `ExperimentBatchConfig`, which specifies the full range of experiments to be performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.solver import generate, prompt_template, system_message\n",
    "\n",
    "from evalsense.generation import GenerationSteps\n",
    "from evalsense.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "from evalsense.tasks import DefaultTaskPreprocessor\n",
    "\n",
    "tasks: list[TaskConfig] = []\n",
    "\n",
    "for type_tier, perturbation_type in enumerate(tiered_perturbation_types):\n",
    "\n",
    "    def perturbation_record_to_sample(\n",
    "        record: dict[str, Any],\n",
    "        type_tier: int = type_tier,\n",
    "    ) -> Sample:\n",
    "        return Sample(\n",
    "            input=record[\"note\"],\n",
    "            target=record[\"note\"],\n",
    "            id=record[\"id\"],\n",
    "            metadata={\n",
    "                \"dialogue\": record[\"dialogue\"],\n",
    "                \"dataset\": record[\"dataset\"],\n",
    "                \"encounter_id\": record[\"encounter_id\"],\n",
    "                \"doctor_name\": record[\"doctor_name\"],\n",
    "                \"patient_gender\": record[\"patient_gender\"],\n",
    "                \"patient_age\": record[\"patient_age\"],\n",
    "                \"patient_firstname\": record[\"patient_firstname\"],\n",
    "                \"patient_familyname\": record[\"patient_familyname\"],\n",
    "                \"cc\": record[\"cc\"],\n",
    "                \"2nd_complaints\": record[\"2nd_complaints\"],\n",
    "                \"perturbation_type_tier\": type_tier,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    user_prompt_template_variant = user_prompt_template.replace(\n",
    "        \"{perturbation_types}\", \"\\n\".join(perturbation_type)\n",
    "    )\n",
    "    perturb_generation = GenerationSteps(\n",
    "        name=f\"Perturbation tier {type_tier + 1}\",\n",
    "        steps=[\n",
    "            system_message(system_prompt_template),\n",
    "            prompt_template(user_prompt_template_variant),\n",
    "            generate(),\n",
    "        ],\n",
    "    )\n",
    "    perturb_task_preprocessor = DefaultTaskPreprocessor(name=\"Perturbation\")\n",
    "\n",
    "    task_config = TaskConfig(\n",
    "        dataset_manager=aci_dataset_manager,\n",
    "        generation_steps=perturb_generation,\n",
    "        field_spec=perturbation_record_to_sample,\n",
    "        task_preprocessor=perturb_task_preprocessor,\n",
    "    )\n",
    "    tasks.append(task_config)\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=tasks,\n",
    "    model_configs=[\n",
    "        gemma_27_config,\n",
    "        qwen_14_config,\n",
    "        llama_config,\n",
    "        phi_config,\n",
    "    ],\n",
    "    evaluators=[\n",
    "        bleu_evaluator,\n",
    "        rouge_evaluator,\n",
    "        bertscore_evaluator,\n",
    "        llama_detailed_g_eval_evaluator,\n",
    "        llama_brief_g_eval_evaluator,\n",
    "        qwen_detailed_g_eval_evaluator,\n",
    "        qwen_brief_g_eval_evaluator,\n",
    "        gemma_detailed_g_eval_evaluator,\n",
    "        gemma_brief_g_eval_evaluator,\n",
    "        llama_ternary_reference_based_qags_evaluator,\n",
    "        llama_judge_reference_based_qags_evaluator,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "To store all the logs, outputs and results associated with our experiments, we also create a new `Project` object, which is passed to the evaluation pipeline. When initialised, the project automatically loads any previously stored outputs and evaluation results assciated with the given project name. This ensures that any completed tasks are not unnecessarily rerun unless explicitly required by setting `force_rerun=True` when calling the pipeline's `run` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Perturbation\")\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "Finally, we can analyse the correlation between the rankings established by the tiered perturbations and the rankings produced by each of the considered evaluation methods. The evaluation methods with the highest correlation can be expected to be the most suitable for evaluating the given criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from evalsense.workflow.analysers import MetaResultAnalyser\n",
    "\n",
    "analyser = MetaResultAnalyser[pl.DataFrame](output_format=\"polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results = analyser(aci_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20, fmt_str_lengths=100):\n",
    "    display(corr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
