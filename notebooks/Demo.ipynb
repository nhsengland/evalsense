{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMScope Demo Notebook\n",
    "\n",
    "This notebooks illustrates a basic application of LLMScope to the ACI-Bench dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using LLMScope, you may need to configure several environment variables, especially when working with local models. Some commonly used variables are described below:\n",
    "\n",
    "* `LLMSCOPE_STORAGE_DIR` — Specifies the directory where LLMScope should store datasets, models, logs ad results. If not set, it defaults to a platform-specific application cache folder.\n",
    "* `TOKENIZERS_PARALLELISM` — It is recommended to set this to `true` when using local models. However, if you encounter issues related to parallelism, you may need to set it to `false`.\n",
    "* `CUDA_HOME` — May be required when using vLLM local models with NVIDIA GPUs. If not already defined in your environment, this should point to the root of your CUDA installation (e.g., `/usr/local/cuda-12.4.0`).\n",
    "* `TORCH_CUDA_ARCH_LIST` — Helps suppress CUDA-related warnings when using local models with NVIDIA GPUs. You can determine the appropriate value(s) by running:\n",
    "    ```\n",
    "    nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLMSCOPE_STORAGE_DIR\"] = \"/vol/bitbucket/ad5518/llmscope_cache\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/vol/cuda/12.4.0/\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "As a first step, we need to define a dataset. In this case, we will use the built-in [ACI-Bench dataset](https://www.nature.com/articles/s41597-023-02487-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Steps\n",
    "The next step is to define how the data should be used to generate model outputs, including the prompts involved. This is achieved using the Solvers abstraction provided by the Inspect AI library. A list of available solvers can be found in the [official documentation](https://inspect.aisi.org.uk/solvers.html#built-in-solvers).\n",
    "\n",
    "Solvers can be composed into a sequence, allowing multiple steps to be chained together, as illustrated below. The full generation steps need to be assigned a unique name — this enables the comparison of results across different experiments when varying prompts or solver configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import generate, prompt_template, system_message\n",
    "\n",
    "from llmscope.generation import GenerationSteps\n",
    "\n",
    "system_prompt_template = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "user_prompt_template = \"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{prompt}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "\n",
    "aci_generation = GenerationSteps(\n",
    "    name=\"Structured\",\n",
    "    solver=[\n",
    "        system_message(system_prompt_template),\n",
    "        prompt_template(user_prompt_template),\n",
    "        generate(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Spec\n",
    "In addition to selecting the dataset to be used, it is also necessary to specify which columns will serve as inputs, targets (i.g., ground-truth references, if available) and sample IDs. Optionally, additional columns can be retained as metadata — for example, if they are useful for the result analysis.\n",
    "\n",
    "Some datasets may support multiple tasks, in which case a preprocessing step may be needed to adapt the data for a specific task. In this case, no task-specific adjustments are needed, so we use the default preprocessor that returns the data unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import FieldSpec\n",
    "\n",
    "from llmscope.tasks import DefaultTaskPreprocessor\n",
    "\n",
    "aci_field_spec = FieldSpec(\n",
    "    input=\"dialogue\",\n",
    "    target=\"note\",\n",
    "    id=\"id\",\n",
    "    metadata=[\n",
    "        \"dataset\",\n",
    "        \"encounter_id\",\n",
    "        \"doctor_name\",\n",
    "        \"patient_gender\",\n",
    "        \"patient_age\",\n",
    "        \"patient_firstname\",\n",
    "        \"patient_familyname\",\n",
    "        \"cc\",\n",
    "        \"2nd_complaints\",\n",
    "    ],\n",
    ")\n",
    "dialogue_task_preprocessor = DefaultTaskPreprocessor(name=\"Dialogue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config\n",
    "Next, we specify the model(s) to be used in our experiment. We can use any model and model provider supported by Inspect AI, configurable through model arguments and generation configuration. For an overview of supported models and the arguments, please refer to the related [Inspect AI documentation](https://inspect.aisi.org.uk/models.html). In this demo, we use two local vLLM models — [Llama 3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B) and [Phi 4 Mini Instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import GenerateConfigArgs\n",
    "\n",
    "from llmscope.constants import MODELS_PATH\n",
    "from llmscope.generation import ModelConfig\n",
    "\n",
    "llama_config = ModelConfig(\n",
    "    \"vllm/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "phi_config = ModelConfig(\n",
    "    \"vllm/microsoft/Phi-4-mini-instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "In this step, we define the evaluation metrics. Here, we simply use the BLUE and ROUGE metrics as implemented in LLMScope. Custom metrics are also supported, provided that they are implemented as Inspect AI-compatible scorers and metrics. For reference and inspiration, please check the [LLMScope BLUE implementation](https://github.com/nhsengland/llmscope/blob/main/llmscope/evaluation/evaluators/bleu.py) and the [relevant Inspect AI documentation](https://inspect.aisi.org.uk/scorers.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation.evaluators import bleu_evaluator\n",
    "from llmscope.evaluation.evaluators import rouge_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "To bring everything together, we configure the overall task and experiment pipeline. For more extensive and complex evaluations, multiple experiment batch configurations can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "\n",
    "aci_task_config = TaskConfig(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    generation_steps=aci_generation,\n",
    "    field_spec=aci_field_spec,\n",
    "    task_preprocessor=dialogue_task_preprocessor,\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=[aci_task_config],\n",
    "    model_configs=[llama_config, phi_config],\n",
    "    evaluators=[bleu_evaluator, rouge_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all the logs, outputs and results associated with our experiments, we also create a new `Project` object, which is passed to the evaluation pipeline. When initialised, the project automatically loads any previously stored outputs and evaluation results assciated with the given project name. This ensures that any completed tasks are not unnecessarily rerun unless explicitly required by setting `force_rerun=True` when calling the pipeline's `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmscope.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Evaluation\")\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "The results associated with the project can be analysed using dedicated result analysers. In this example, we use a simple tabular result analyser, which summarises all the evaluation metrics in a structured table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from llmscope.workflow.analysers import TabularResultAnalyser\n",
    "\n",
    "analyser = TabularResultAnalyser[pl.DataFrame](output_format=\"polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser(aci_project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
