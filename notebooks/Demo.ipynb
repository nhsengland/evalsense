{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvalSense Demo Notebook\n",
    "\n",
    "This notebooks illustrates a basic application of EvalSense to the ACI-Bench dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using EvalSense, you may need to configure several environment variables, especially when working with local models. Some commonly used variables are described below:\n",
    "\n",
    "- `EVALSENSE_STORAGE_DIR` — Specifies the directory where EvalSense should store datasets, models, logs ad results. If not set, it defaults to a platform-specific application cache folder.\n",
    "- `CUDA_HOME` — May be required when using vLLM local models with NVIDIA GPUs. If not already defined in your environment, this should point to the root of your CUDA installation (e.g., `/usr/local/cuda-12.4.0`).\n",
    "- `TORCH_CUDA_ARCH_LIST` — Helps suppress CUDA-related warnings when using local models with NVIDIA GPUs. You can determine the appropriate value(s) by running:\n",
    "  ```\n",
    "  nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "  ```\n",
    "- `TOKENIZERS_PARALLELISM` — It is recommended to set this to `true` when using local models. However, if you encounter issues related to parallelism, you may need to set it to `false`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"EVALSENSE_STORAGE_DIR\"] = \"/vol/bitbucket/ad5518/evalsense_cache\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/vol/cuda/12.4.0/\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "As a first step, we need to define a dataset. In this case, we will use the built-in [ACI-Bench dataset](https://www.nature.com/articles/s41597-023-02487-3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.datasets.managers import AciBenchDatasetManager\n",
    "\n",
    "aci_dataset_manager = AciBenchDatasetManager(splits=[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Steps\n",
    "\n",
    "The next step is to define how the data should be used to generate model outputs, including the prompts involved. This is achieved using the Solvers abstraction provided by the Inspect AI library. A list of available solvers can be found in the [official documentation](https://inspect.aisi.org.uk/solvers.html#built-in-solvers).\n",
    "\n",
    "Solvers can be composed into a sequence, allowing multiple steps to be chained together, as illustrated below. The full generation steps need to be assigned a unique name — this enables the comparison of results across different experiments when varying prompts or solver configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import generate, prompt_template, system_message\n",
    "\n",
    "from evalsense.generation import GenerationSteps\n",
    "\n",
    "system_prompt_template = \"You are an expert clinical assistant specialising in the creation of medically accurate summaries from a dialogue between the doctor and patient.\"\n",
    "user_prompt_template = \"\"\"Your task is to generate a clinical note based on a conversation between a doctor and a patient. Use the following format for the clinical note:\n",
    "\n",
    "1. **CHIEF COMPLAINT**: [Brief description of the main reason for the visit]\n",
    "2. **HISTORY OF PRESENT ILLNESS**: [Summary of the patient's current health status and any changes since the last visit]\n",
    "3. **REVIEW OF SYSTEMS**: [List of symptoms reported by the patient]\n",
    "4. **PHYSICAL EXAMINATION**: [Findings from the physical examination]\n",
    "5. **RESULTS**: [Relevant test results]\n",
    "6. **ASSESSMENT AND PLAN**: [Doctor's assessment and plan for treatment or further testing]\n",
    "\n",
    "**Conversation:**\n",
    "{prompt}\n",
    "\n",
    "**Note:**\n",
    "\"\"\"\n",
    "\n",
    "aci_generation = GenerationSteps(\n",
    "    name=\"Structured\",\n",
    "    steps=[\n",
    "        system_message(system_prompt_template),\n",
    "        prompt_template(user_prompt_template),\n",
    "        generate(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Spec\n",
    "\n",
    "In addition to selecting the dataset to be used, it is also necessary to specify which columns will serve as inputs, targets (i.g., ground-truth references, if available) and sample IDs. Optionally, additional columns can be retained as metadata — for example, if they are useful for the result analysis.\n",
    "\n",
    "Some datasets may support multiple tasks, in which case a preprocessing step may be needed to adapt the data for a specific task. In this case, no task-specific adjustments are needed, so we use the default preprocessor that returns the data unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import FieldSpec\n",
    "\n",
    "from evalsense.tasks import DefaultTaskPreprocessor\n",
    "\n",
    "aci_field_spec = FieldSpec(\n",
    "    input=\"dialogue\",\n",
    "    target=\"note\",\n",
    "    id=\"id\",\n",
    "    metadata=[\n",
    "        \"dataset\",\n",
    "        \"encounter_id\",\n",
    "        \"doctor_name\",\n",
    "        \"patient_gender\",\n",
    "        \"patient_age\",\n",
    "        \"patient_firstname\",\n",
    "        \"patient_familyname\",\n",
    "        \"cc\",\n",
    "        \"2nd_complaints\",\n",
    "    ],\n",
    ")\n",
    "dialogue_task_preprocessor = DefaultTaskPreprocessor(name=\"Dialogue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config\n",
    "\n",
    "Next, we specify the model(s) to be used in our experiment. We can use any model and model provider supported by Inspect AI, configurable through model arguments and generation configuration. For an overview of supported models and the arguments, please refer to the related [Inspect AI documentation](https://inspect.aisi.org.uk/models.html). In this demo, we use two local vLLM models — [Llama 3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B) and [Phi 4 Mini Instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.model import GenerateConfigArgs\n",
    "\n",
    "from evalsense.constants import MODELS_PATH\n",
    "from evalsense.generation import ModelConfig\n",
    "\n",
    "llama_config = ModelConfig(\n",
    "    \"vllm/meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "phi_config = ModelConfig(\n",
    "    \"vllm/microsoft/Phi-4-mini-instruct\",\n",
    "    model_args={\n",
    "        \"download_dir\": MODELS_PATH,\n",
    "        \"device\": \"2\",\n",
    "        \"gpu_memory_utilization\": 0.8,\n",
    "        \"max_model_len\": 8192,\n",
    "    },\n",
    "    generation_args=GenerateConfigArgs(\n",
    "        seed=42,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_connections=128,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this step, we define the evaluation metrics. Here, we simply use the BLUE, ROUGE and BERTScore metrics as implemented in EvalSense. Custom metrics are also supported, provided that they are implemented as Inspect AI-compatible [scorers](https://inspect.aisi.org.uk/scorers.html#custom-scorers) and [metrics](https://inspect.aisi.org.uk/reference/inspect_ai.scorer.html#metric). For reference and inspiration, please check the [EvalSense BLUE implementation](https://github.com/nhsengland/evalsense/blob/main/evalsense/evaluation/evaluators/bleu.py) and the [relevant Inspect AI documentation](https://inspect.aisi.org.uk/scorers.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import (\n",
    "    get_bertscore_evaluator,\n",
    "    get_bleu_evaluator,\n",
    "    get_rouge_evaluator,\n",
    ")\n",
    "\n",
    "bleu_evaluator = get_bleu_evaluator()\n",
    "rouge_evaluator = get_rouge_evaluator()\n",
    "bertscore_evaluator = get_bertscore_evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the use of more advanced metrics, we also include [G-Eval](https://arxiv.org/abs/2303.16634), a LLM-as-a-Judge approach that uses structured propts and weights judge scores based on token output proabilities, resulting in more stable evaluation scores. Unlike simpler metrics, G-Eval requires additional configuration, such as a function for constructing the prompts as well as additional hyperparameters.\n",
    "\n",
    "Since G-Eval is a model-based metric, we also need to supply a model configuration so that EvalSense can instantiate the appropriate model during evaluation. Internally, EvalSense handles this evaluator differently from simpler metrics by using a `ScorerFactory` — an abstraction that creates a scorer when passed an Inspect AI model. This design allows LLMScore to optimise resource usage by loading the model only when needed and releasing the associated resources afterwards.\n",
    "\n",
    "While you don't need to worry about these internal details if you are only using default EvalSense evaluators, understanding them can be useful if you want to implement your own model-based evaluation methods. For reference, you can check the [EvalSense implementation of G-Eval](https://github.com/nhsengland/evalsense/blob/main/evalsense/evaluation/evaluators/g_eval.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation.evaluators import get_g_eval_evaluator\n",
    "\n",
    "prompt = \"\"\"You are a medical expert tasked with evaluating the faithfulness of a clinical note generated by a model from doctor-patient dialogue. You will be given:\n",
    "\n",
    "* The reference note produced by a human expert\n",
    "* The candidate note produced by the model\n",
    "\n",
    "Your goal is to determine whether the candidate summary is faithful to the reference note, using the following evaluation criteria:\n",
    "\n",
    "1. **Accuracy of Medical Facts**: All medical conditions, diagnoses, treatments, and test results must be correctly stated and consistent with the source text.\n",
    "2. **Completeness of Critical Information**: The summary should include all vital information necessary for follow-up care (e.g., key symptoms, diagnoses, procedures, outcomes).\n",
    "3. **Absence of Hallucinations**: The summary should not introduce any information that is not present in the original discharge note.\n",
    "4. **Clarity and Non-Misleading Content**: The summary should be clear, free of ambiguity, and should not distort or misrepresent any facts.\n",
    "\n",
    "Instructions:\n",
    "* Compare the candidate summary to the reference summary.\n",
    "* Provide a numerical rating of the candidate summary's faithfulness on a scale from 1 (completely unfaithful) to 10 (fully faithful).\n",
    "* Respond only with the numerical rating without any explanation or context.\n",
    "\n",
    "Reference Note:\n",
    "\n",
    "{reference}\n",
    "\n",
    "Candidate Note:\n",
    "\n",
    "{prediction}\n",
    "\n",
    "Output Format:\n",
    "[Numerical faithfulness rating only, from 1 to 10]\"\"\"\n",
    "\n",
    "\n",
    "g_eval_evaluator = get_g_eval_evaluator(\n",
    "    quality_name=\"Faithfulness\",\n",
    "    prompt_template=prompt,\n",
    "    model_config=llama_config,\n",
    "    min_score=1,\n",
    "    max_score=10,\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "To bring everything together, we configure the overall task and experiment pipeline. For more extensive and complex evaluations, multiple experiment batch configurations can be specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.evaluation import ExperimentBatchConfig, TaskConfig\n",
    "\n",
    "aci_task_config = TaskConfig(\n",
    "    dataset_manager=aci_dataset_manager,\n",
    "    generation_steps=aci_generation,\n",
    "    field_spec=aci_field_spec,\n",
    "    task_preprocessor=dialogue_task_preprocessor,\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentBatchConfig(\n",
    "    tasks=[aci_task_config],\n",
    "    model_configs=[llama_config, phi_config],\n",
    "    evaluators=[bleu_evaluator, rouge_evaluator, bertscore_evaluator, g_eval_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all the logs, outputs and results associated with our experiments, we also create a new `Project` object, which is passed to the evaluation pipeline. When initialised, the project automatically loads any previously stored outputs and evaluation results assciated with the given project name. This ensures that any completed tasks are not unnecessarily rerun unless explicitly required by setting `force_rerun=True` when calling the pipeline's `run` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalsense.workflow import Pipeline, Project\n",
    "\n",
    "aci_project = Project(name=\"ACI-Bench Demo\")\n",
    "\n",
    "aci_pipeline = Pipeline(\n",
    "    experiments=experiment_config,\n",
    "    project=aci_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "The results associated with the project can be analysed using dedicated result analysers. In this example, we use a simple tabular result analyser, which summarises all the evaluation metrics in a structured table format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from evalsense.workflow.analysers import TabularResultAnalyser\n",
    "\n",
    "analyser = TabularResultAnalyser[pl.DataFrame](output_format=\"polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser(aci_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
